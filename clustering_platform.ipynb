{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clustering Platform\n",
        "\n",
        "An interactive platform for testing various clustering algorithms with support for multiple use cases, data formats (JSON/CSV), and guided workflows.\n",
        "\n",
        "## Features\n",
        "- Use Case Management\n",
        "- Guided Questionnaire\n",
        "- Multiple Data Formats (CSV/JSON)\n",
        "- Algorithm Selection (K-Means, DBSCAN, HDBSCAN, Hierarchical, GMM)\n",
        "- Interactive Visualization\n",
        "- Comprehensive Metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup & Imports\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Widgets\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual, Layout\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# Clustering algorithms\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import hdbscan\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✓ All libraries imported successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Use Case Management\n",
        "\n",
        "Select an existing use case or create a new one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Case Management Functions\n",
        "\n",
        "USE_CASES_DIR = Path(\"use_cases\")\n",
        "USE_CASES_FILE = USE_CASES_DIR / \"use_cases.json\"\n",
        "\n",
        "# Global state\n",
        "current_use_case = None\n",
        "use_cases_registry = {}\n",
        "questionnaire_answers = {}\n",
        "uploaded_data = None\n",
        "processed_data = None\n",
        "selected_features = []\n",
        "clustering_results = {}\n",
        "\n",
        "def load_use_cases():\n",
        "    \"\"\"Load use cases from registry file\"\"\"\n",
        "    global use_cases_registry\n",
        "    try:\n",
        "        if USE_CASES_FILE.exists():\n",
        "            with open(USE_CASES_FILE, 'r') as f:\n",
        "                use_cases_registry = json.load(f)\n",
        "            return use_cases_registry\n",
        "        else:\n",
        "            print(f\"Warning: {USE_CASES_FILE} not found. Creating default registry.\")\n",
        "            return {}\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading use cases: {e}\")\n",
        "        return {}\n",
        "\n",
        "def save_use_case(use_case_id, use_case_data):\n",
        "    \"\"\"Save a new use case to the registry\"\"\"\n",
        "    global use_cases_registry\n",
        "    use_cases_registry[use_case_id] = use_case_data\n",
        "    try:\n",
        "        USE_CASES_DIR.mkdir(exist_ok=True)\n",
        "        with open(USE_CASES_FILE, 'w') as f:\n",
        "            json.dump(use_cases_registry, f, indent=2)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving use case: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_use_case_list():\n",
        "    \"\"\"Get list of use case names for dropdown\"\"\"\n",
        "    use_cases = load_use_cases()\n",
        "    return [\"Create New Use Case\"] + [use_cases[uc_id][\"name\"] for uc_id in use_cases.keys()]\n",
        "\n",
        "# Load use cases\n",
        "use_cases_registry = load_use_cases()\n",
        "print(f\"✓ Loaded {len(use_cases_registry)} use case(s)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Case Selection UI\n",
        "\n",
        "def get_use_case_id_by_name(name):\n",
        "    \"\"\"Get use case ID from name\"\"\"\n",
        "    for uc_id, uc_data in use_cases_registry.items():\n",
        "        if uc_data[\"name\"] == name:\n",
        "            return uc_id\n",
        "    return None\n",
        "\n",
        "def on_use_case_change(change):\n",
        "    \"\"\"Handle use case selection change\"\"\"\n",
        "    global current_use_case\n",
        "    if change['new'] == \"Create New Use Case\":\n",
        "        current_use_case = None\n",
        "        new_use_case_ui.layout.display = 'flex'\n",
        "        use_case_info_ui.layout.display = 'none'\n",
        "    else:\n",
        "        uc_id = get_use_case_id_by_name(change['new'])\n",
        "        if uc_id:\n",
        "            current_use_case = uc_id\n",
        "            use_case_info_ui.value = f\"\"\"\n",
        "            <h4>{use_cases_registry[uc_id]['name']}</h4>\n",
        "            <p><strong>Description:</strong> {use_cases_registry[uc_id]['description']}</p>\n",
        "            <p><strong>Recommended Algorithms:</strong> {', '.join(use_cases_registry[uc_id]['recommended_algorithms'])}</p>\n",
        "            \"\"\"\n",
        "            use_case_info_ui.layout.display = 'flex'\n",
        "            new_use_case_ui.layout.display = 'none'\n",
        "        else:\n",
        "            current_use_case = None\n",
        "\n",
        "# Use Case Selection Widgets\n",
        "use_case_dropdown = widgets.Dropdown(\n",
        "    options=get_use_case_list(),\n",
        "    description='Use Case:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='400px')\n",
        ")\n",
        "\n",
        "use_case_info_ui = widgets.HTML(\n",
        "    value=\"\",\n",
        "    layout=Layout(display='none', margin='10px 0px')\n",
        ")\n",
        "\n",
        "new_use_case_name = widgets.Text(\n",
        "    description='Name:',\n",
        "    placeholder='e.g., Network Traffic Clustering',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='400px')\n",
        ")\n",
        "\n",
        "new_use_case_desc = widgets.Textarea(\n",
        "    description='Description:',\n",
        "    placeholder='Describe your clustering use case...',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='400px', height='100px')\n",
        ")\n",
        "\n",
        "def create_new_use_case(b):\n",
        "    \"\"\"Create a new use case\"\"\"\n",
        "    name = new_use_case_name.value.strip()\n",
        "    desc = new_use_case_desc.value.strip()\n",
        "    if not name:\n",
        "        print(\"Error: Use case name is required\")\n",
        "        return\n",
        "    \n",
        "    uc_id = name.lower().replace(' ', '_')\n",
        "    new_use_case = {\n",
        "        \"name\": name,\n",
        "        \"description\": desc,\n",
        "        \"data_format\": [\"json\", \"csv\"],\n",
        "        \"recommended_algorithms\": [\"K-Means\", \"HDBSCAN\"],\n",
        "        \"typical_features\": [],\n",
        "        \"questions\": [],\n",
        "        \"default_preprocessing\": {\n",
        "            \"handle_missing\": \"drop\",\n",
        "            \"scale_features\": True,\n",
        "            \"scaler_type\": \"StandardScaler\",\n",
        "            \"encode_categorical\": True\n",
        "        },\n",
        "        \"default_parameters\": {}\n",
        "    }\n",
        "    \n",
        "    if save_use_case(uc_id, new_use_case):\n",
        "        global use_cases_registry\n",
        "        use_cases_registry = load_use_cases()\n",
        "        use_case_dropdown.options = get_use_case_list()\n",
        "        use_case_dropdown.value = name\n",
        "        print(f\"✓ Created new use case: {name}\")\n",
        "        new_use_case_name.value = \"\"\n",
        "        new_use_case_desc.value = \"\"\n",
        "\n",
        "create_use_case_btn = widgets.Button(\n",
        "    description='Create Use Case',\n",
        "    button_style='success',\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "create_use_case_btn.on_click(create_new_use_case)\n",
        "\n",
        "new_use_case_ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h4>Create New Use Case</h4>\"),\n",
        "    new_use_case_name,\n",
        "    new_use_case_desc,\n",
        "    create_use_case_btn\n",
        "], layout=Layout(display='none', border='1px solid #ccc', padding='10px', margin='10px 0px'))\n",
        "\n",
        "use_case_dropdown.observe(on_use_case_change, names='value')\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>Use Case Selection</h3>\"),\n",
        "    use_case_dropdown,\n",
        "    use_case_info_ui,\n",
        "    new_use_case_ui\n",
        "]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Guided Questionnaire\n",
        "\n",
        "Answer questions about your clustering goals to get algorithm recommendations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Questionnaire Functions\n",
        "\n",
        "def get_use_case_questions():\n",
        "    \"\"\"Get questions for current use case\"\"\"\n",
        "    if current_use_case and current_use_case in use_cases_registry:\n",
        "        return use_cases_registry[current_use_case].get(\"questions\", [])\n",
        "    # Default questions for generic use case\n",
        "    return use_cases_registry.get(\"generic\", {}).get(\"questions\", [])\n",
        "\n",
        "def render_question_widget(question):\n",
        "    \"\"\"Render a widget for a question based on its type\"\"\"\n",
        "    q_id = question.get(\"id\", \"\")\n",
        "    q_text = question.get(\"question\", \"\")\n",
        "    q_type = question.get(\"type\", \"text\")\n",
        "    q_options = question.get(\"options\", [])\n",
        "    q_default = question.get(\"default\", \"\")\n",
        "    q_required = question.get(\"required\", False)\n",
        "    \n",
        "    if q_type == \"dropdown\":\n",
        "        widget = widgets.Dropdown(\n",
        "            options=q_options,\n",
        "            description=q_text + (\" *\" if q_required else \"\"),\n",
        "            value=q_default if q_default in q_options else q_options[0] if q_options else \"\",\n",
        "            style={'description_width': 'initial'},\n",
        "            layout=Layout(width='500px')\n",
        "        )\n",
        "    elif q_type == \"text\":\n",
        "        widget = widgets.Text(\n",
        "            description=q_text + (\" *\" if q_required else \"\"),\n",
        "            value=q_default,\n",
        "            placeholder=\"Enter your answer...\",\n",
        "            style={'description_width': 'initial'},\n",
        "            layout=Layout(width='500px')\n",
        "        )\n",
        "    elif q_type == \"textarea\":\n",
        "        widget = widgets.Textarea(\n",
        "            description=q_text + (\" *\" if q_required else \"\"),\n",
        "            value=q_default,\n",
        "            placeholder=\"Enter your answer...\",\n",
        "            style={'description_width': 'initial'},\n",
        "            layout=Layout(width='500px', height='100px')\n",
        "        )\n",
        "    else:\n",
        "        widget = widgets.Text(\n",
        "            description=q_text + (\" *\" if q_required else \"\"),\n",
        "            value=q_default,\n",
        "            style={'description_width': 'initial'},\n",
        "            layout=Layout(width='500px')\n",
        "        )\n",
        "    \n",
        "    widget.question_id = q_id\n",
        "    widget.question_required = q_required\n",
        "    return widget\n",
        "\n",
        "def collect_questionnaire_answers(question_widgets):\n",
        "    \"\"\"Collect answers from questionnaire widgets\"\"\"\n",
        "    answers = {}\n",
        "    for widget in question_widgets:\n",
        "        if hasattr(widget, 'question_id'):\n",
        "            value = widget.value\n",
        "            if isinstance(value, str):\n",
        "                value = value.strip()\n",
        "            if widget.question_required and (not value or value == \"\"):\n",
        "                return None, f\"Required question '{widget.description}' not answered\"\n",
        "            answers[widget.question_id] = value\n",
        "    return answers, None\n",
        "\n",
        "def recommend_algorithms(answers):\n",
        "    \"\"\"Recommend algorithms based on questionnaire answers\"\"\"\n",
        "    recommendations = []\n",
        "    reasoning = []\n",
        "    \n",
        "    noise_level = answers.get(\"noise_level\", \"Medium\").lower()\n",
        "    known_cluster_count = answers.get(\"known_cluster_count\", \"No\").lower()\n",
        "    known_entity_count = answers.get(\"known_entity_count\", \"No\").lower()\n",
        "    cluster_size_variation = answers.get(\"cluster_size_variation\", \"Unknown\").lower()\n",
        "    \n",
        "    # High noise -> HDBSCAN or DBSCAN\n",
        "    if \"high\" in noise_level:\n",
        "        recommendations.append((\"HDBSCAN\", 5))\n",
        "        recommendations.append((\"DBSCAN\", 4))\n",
        "        reasoning.append(\"High noise level detected - density-based algorithms recommended\")\n",
        "    \n",
        "    # Unknown cluster count -> HDBSCAN, DBSCAN, or Hierarchical\n",
        "    if \"no\" in known_cluster_count or \"no\" in known_entity_count:\n",
        "        if (\"HDBSCAN\", 5) not in recommendations:\n",
        "            recommendations.append((\"HDBSCAN\", 5))\n",
        "        if (\"DBSCAN\", 4) not in recommendations:\n",
        "            recommendations.append((\"DBSCAN\", 4))\n",
        "        recommendations.append((\"Hierarchical\", 3))\n",
        "        reasoning.append(\"Unknown cluster count - algorithms that don't require cluster count recommended\")\n",
        "    \n",
        "    # Known cluster count -> K-Means or GMM\n",
        "    if \"yes\" in known_cluster_count or \"yes\" in known_entity_count:\n",
        "        recommendations.append((\"K-Means\", 4))\n",
        "        recommendations.append((\"GMM\", 3))\n",
        "        reasoning.append(\"Known cluster count - centroid/probabilistic algorithms suitable\")\n",
        "    \n",
        "    # Varying cluster sizes -> HDBSCAN or Hierarchical\n",
        "    if \"no\" in cluster_size_variation:\n",
        "        if (\"HDBSCAN\", 5) not in recommendations:\n",
        "            recommendations.append((\"HDBSCAN\", 4))\n",
        "        recommendations.append((\"Hierarchical\", 4))\n",
        "        reasoning.append(\"Varying cluster sizes expected - hierarchical methods recommended\")\n",
        "    \n",
        "    # Similar cluster sizes -> K-Means or GMM\n",
        "    if \"yes\" in cluster_size_variation:\n",
        "        recommendations.append((\"K-Means\", 4))\n",
        "        recommendations.append((\"GMM\", 3))\n",
        "    \n",
        "    # Remove duplicates and sort by score\n",
        "    seen = set()\n",
        "    unique_recs = []\n",
        "    for alg, score in recommendations:\n",
        "        if alg not in seen:\n",
        "            seen.add(alg)\n",
        "            unique_recs.append((alg, score))\n",
        "    \n",
        "    unique_recs.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    # Get top 3\n",
        "    top_recs = unique_recs[:3]\n",
        "    \n",
        "    return [alg for alg, _ in top_recs], reasoning\n",
        "\n",
        "question_widgets = []\n",
        "questionnaire_container = widgets.VBox([])\n",
        "\n",
        "def on_submit_questionnaire(b):\n",
        "    \"\"\"Handle questionnaire submission\"\"\"\n",
        "    global questionnaire_answers\n",
        "    \n",
        "    answers, error = collect_questionnaire_answers(question_widgets)\n",
        "    if error:\n",
        "        print(f\"Error: {error}\")\n",
        "        return\n",
        "    \n",
        "    questionnaire_answers = answers\n",
        "    recommendations, reasoning = recommend_algorithms(answers)\n",
        "    \n",
        "    rec_text = \"<h4>Algorithm Recommendations:</h4><ul>\"\n",
        "    for rec in recommendations:\n",
        "        rec_text += f\"<li><strong>{rec}</strong></li>\"\n",
        "    rec_text += \"</ul>\"\n",
        "    \n",
        "    if reasoning:\n",
        "        rec_text += \"<h5>Reasoning:</h5><ul>\"\n",
        "        for reason in reasoning:\n",
        "            rec_text += f\"<li>{reason}</li>\"\n",
        "        rec_text += \"</ul>\"\n",
        "    \n",
        "    recommendation_display.value = rec_text\n",
        "    recommendation_display.layout.display = 'flex'\n",
        "    print(\"✓ Questionnaire submitted successfully!\")\n",
        "\n",
        "# Create questionnaire UI\n",
        "recommendation_display = widgets.HTML(\n",
        "    value=\"\",\n",
        "    layout=Layout(display='none', border='1px solid #4CAF50', padding='10px', margin='10px 0px')\n",
        ")\n",
        "\n",
        "submit_btn = widgets.Button(\n",
        "    description='Submit Answers',\n",
        "    button_style='primary',\n",
        "    layout=Layout(width='200px', margin='10px 0px')\n",
        ")\n",
        "submit_btn.on_click(on_submit_questionnaire)\n",
        "\n",
        "def update_questionnaire():\n",
        "    \"\"\"Update questionnaire based on selected use case\"\"\"\n",
        "    global question_widgets, questionnaire_container\n",
        "    \n",
        "    questions = get_use_case_questions()\n",
        "    if not questions:\n",
        "        questionnaire_container.children = [\n",
        "            widgets.HTML(\"<p>No questions defined for this use case. You can proceed to data upload.</p>\")\n",
        "        ]\n",
        "        return\n",
        "    \n",
        "    question_widgets = [render_question_widget(q) for q in questions]\n",
        "    \n",
        "    questionnaire_container.children = [\n",
        "        widgets.HTML(\"<h4>Please answer the following questions:</h4>\")\n",
        "    ] + question_widgets + [\n",
        "        submit_btn\n",
        "    ]\n",
        "\n",
        "# Auto-update questionnaire when use case changes\n",
        "def on_use_case_change_for_questionnaire(change):\n",
        "    update_questionnaire()\n",
        "\n",
        "# Note: use_case_dropdown is defined in cell 4, so this observer will be set up there\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>Guided Questionnaire</h3>\"),\n",
        "    questionnaire_container,\n",
        "    recommendation_display\n",
        "]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JSON Parsing Utilities\n",
        "\n",
        "def detect_json_structure(json_data):\n",
        "    \"\"\"Detect JSON structure type\"\"\"\n",
        "    if isinstance(json_data, list):\n",
        "        if len(json_data) > 0 and isinstance(json_data[0], dict):\n",
        "            return \"array_of_objects\"\n",
        "        return \"array\"\n",
        "    elif isinstance(json_data, dict):\n",
        "        # Check if it's JSON Lines (multiple objects as string)\n",
        "        if len(json_data) == 1 and isinstance(list(json_data.values())[0], list):\n",
        "            return \"nested_objects\"\n",
        "        return \"object\"\n",
        "    return \"unknown\"\n",
        "\n",
        "def flatten_dict(d, parent_key='', sep='_'):\n",
        "    \"\"\"Flatten a nested dictionary\"\"\"\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        elif isinstance(v, list):\n",
        "            # Handle lists: convert to string or extract if single element\n",
        "            if len(v) == 0:\n",
        "                items.append((new_key, None))\n",
        "            elif len(v) == 1:\n",
        "                if isinstance(v[0], dict):\n",
        "                    items.extend(flatten_dict(v[0], new_key, sep=sep).items())\n",
        "                else:\n",
        "                    items.append((new_key, v[0]))\n",
        "            else:\n",
        "                # Multiple elements: join as string or create multiple columns\n",
        "                items.append((new_key, ', '.join(str(x) for x in v)))\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "def parse_json_data(json_content):\n",
        "    \"\"\"Parse JSON content and convert to DataFrame\"\"\"\n",
        "    try:\n",
        "        # Try parsing as JSON\n",
        "        if isinstance(json_content, bytes):\n",
        "            json_content = json_content.decode('utf-8')\n",
        "        \n",
        "        # Try JSON Lines format first\n",
        "        if '\\n' in json_content and json_content.strip().startswith('{'):\n",
        "            lines = [line.strip() for line in json_content.strip().split('\\n') if line.strip()]\n",
        "            json_objects = []\n",
        "            for line in lines:\n",
        "                try:\n",
        "                    json_objects.append(json.loads(line))\n",
        "                except:\n",
        "                    pass\n",
        "            if json_objects:\n",
        "                json_data = json_objects\n",
        "            else:\n",
        "                json_data = json.loads(json_content)\n",
        "        else:\n",
        "            json_data = json.loads(json_content)\n",
        "        \n",
        "        # Detect structure\n",
        "        structure_type = detect_json_structure(json_data)\n",
        "        \n",
        "        if structure_type == \"array_of_objects\":\n",
        "            # Flatten each object\n",
        "            flattened = [flatten_dict(obj) for obj in json_data]\n",
        "            df = pd.DataFrame(flattened)\n",
        "        elif structure_type == \"nested_objects\":\n",
        "            # Handle nested structure\n",
        "            flattened = flatten_dict(json_data)\n",
        "            df = pd.DataFrame([flattened])\n",
        "        elif structure_type == \"object\":\n",
        "            # Single object\n",
        "            flattened = flatten_dict(json_data)\n",
        "            df = pd.DataFrame([flattened])\n",
        "        else:\n",
        "            df = pd.DataFrame(json_data)\n",
        "        \n",
        "        return df, None\n",
        "        \n",
        "    except json.JSONDecodeError as e:\n",
        "        return None, f\"Invalid JSON format: {e}\"\n",
        "    except Exception as e:\n",
        "        return None, f\"Error parsing JSON: {e}\"\n",
        "\n",
        "def parse_csv_data(csv_content):\n",
        "    \"\"\"Parse CSV content and convert to DataFrame\"\"\"\n",
        "    try:\n",
        "        if isinstance(csv_content, bytes):\n",
        "            csv_content = csv_content.decode('utf-8')\n",
        "        \n",
        "        from io import StringIO\n",
        "        df = pd.read_csv(StringIO(csv_content))\n",
        "        return df, None\n",
        "    except Exception as e:\n",
        "        return None, f\"Error parsing CSV: {e}\"\n",
        "\n",
        "print(\"✓ JSON parsing utilities loaded\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Upload UI\n",
        "\n",
        "data_preview = widgets.Output(layout=Layout(height='300px', overflow='auto'))\n",
        "data_info = widgets.HTML(value=\"\", layout=Layout(margin='10px 0px'))\n",
        "\n",
        "def handle_upload(change):\n",
        "    \"\"\"Handle file upload\"\"\"\n",
        "    global uploaded_data\n",
        "    \n",
        "    if len(change['new']) == 0:\n",
        "        return\n",
        "    \n",
        "    uploaded_file = change['new'][0]\n",
        "    file_name = uploaded_file['name']\n",
        "    file_content = uploaded_file['content']\n",
        "    \n",
        "    with data_preview:\n",
        "        clear_output()\n",
        "        \n",
        "        # Detect file type\n",
        "        if file_name.endswith('.json'):\n",
        "            df, error = parse_json_data(file_content)\n",
        "            file_type = \"JSON\"\n",
        "        elif file_name.endswith('.csv'):\n",
        "            df, error = parse_csv_data(file_content)\n",
        "            file_type = \"CSV\"\n",
        "        else:\n",
        "            # Try to detect by content\n",
        "            try:\n",
        "                if isinstance(file_content, bytes):\n",
        "                    content_str = file_content.decode('utf-8')\n",
        "                else:\n",
        "                    content_str = file_content\n",
        "                \n",
        "                if content_str.strip().startswith('{') or content_str.strip().startswith('['):\n",
        "                    df, error = parse_json_data(content_str)\n",
        "                    file_type = \"JSON\"\n",
        "                else:\n",
        "                    df, error = parse_csv_data(content_str)\n",
        "                    file_type = \"CSV\"\n",
        "            except:\n",
        "                print(f\"Error: Could not determine file type for {file_name}\")\n",
        "                return\n",
        "        \n",
        "        if error:\n",
        "            print(f\"Error: {error}\")\n",
        "            return\n",
        "        \n",
        "        uploaded_data = df\n",
        "        \n",
        "        # Update feature selection\n",
        "        update_feature_selection()\n",
        "        \n",
        "        # Display data info\n",
        "        info_text = f\"\"\"\n",
        "        <h4>Data Uploaded Successfully!</h4>\n",
        "        <p><strong>File:</strong> {file_name}</p>\n",
        "        <p><strong>Type:</strong> {file_type}</p>\n",
        "        <p><strong>Shape:</strong> {df.shape[0]} rows × {df.shape[1]} columns</p>\n",
        "        <p><strong>Columns:</strong> {', '.join(df.columns.tolist()[:10])}{'...' if len(df.columns) > 10 else ''}</p>\n",
        "        \"\"\"\n",
        "        data_info.value = info_text\n",
        "        \n",
        "        # Display preview\n",
        "        print(f\"Data Preview (first 10 rows):\")\n",
        "        display(df.head(10))\n",
        "        print(f\"\\nData Types:\")\n",
        "        display(df.dtypes)\n",
        "        print(f\"\\nMissing Values:\")\n",
        "        display(df.isnull().sum())\n",
        "\n",
        "file_upload = widgets.FileUpload(\n",
        "    accept='.csv,.json',\n",
        "    multiple=False,\n",
        "    description='Upload Data',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "file_upload.observe(handle_upload, names='value')\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>Data Upload</h3>\"),\n",
        "    file_upload,\n",
        "    data_info,\n",
        "    data_preview\n",
        "]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing\n",
        "\n",
        "Select features and configure preprocessing options.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing Functions\n",
        "\n",
        "def get_numeric_columns(df):\n",
        "    \"\"\"Get numeric columns from DataFrame\"\"\"\n",
        "    return df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "def get_categorical_columns(df):\n",
        "    \"\"\"Get categorical columns from DataFrame\"\"\"\n",
        "    return df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "def preprocess_data(df, selected_features, handle_missing='drop', scale_features=True, \n",
        "                   scaler_type='StandardScaler', encode_categorical=True):\n",
        "    \"\"\"Preprocess data for clustering\"\"\"\n",
        "    global processed_data\n",
        "    \n",
        "    if df is None or len(selected_features) == 0:\n",
        "        return None, \"No data or features selected\"\n",
        "    \n",
        "    # Select features\n",
        "    data = df[selected_features].copy()\n",
        "    \n",
        "    # Handle missing values\n",
        "    if handle_missing == 'drop':\n",
        "        data = data.dropna()\n",
        "    elif handle_missing == 'mean':\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        numeric_cols = get_numeric_columns(data)\n",
        "        if numeric_cols:\n",
        "            data[numeric_cols] = imputer.fit_transform(data[numeric_cols])\n",
        "    elif handle_missing == 'median':\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        numeric_cols = get_numeric_columns(data)\n",
        "        if numeric_cols:\n",
        "            data[numeric_cols] = imputer.fit_transform(data[numeric_cols])\n",
        "    \n",
        "    # Encode categorical variables\n",
        "    if encode_categorical:\n",
        "        categorical_cols = get_categorical_columns(data)\n",
        "        label_encoders = {}\n",
        "        for col in categorical_cols:\n",
        "            le = LabelEncoder()\n",
        "            data[col] = le.fit_transform(data[col].astype(str))\n",
        "            label_encoders[col] = le\n",
        "    \n",
        "    # Convert to numeric (handle any remaining non-numeric)\n",
        "    for col in data.columns:\n",
        "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "    \n",
        "    # Drop any remaining NaN\n",
        "    data = data.dropna()\n",
        "    \n",
        "    # Scale features\n",
        "    if scale_features:\n",
        "        if scaler_type == 'StandardScaler':\n",
        "            scaler = StandardScaler()\n",
        "        elif scaler_type == 'MinMaxScaler':\n",
        "            scaler = MinMaxScaler()\n",
        "        else:\n",
        "            scaler = StandardScaler()\n",
        "        \n",
        "        scaled_data = scaler.fit_transform(data)\n",
        "        processed_data = pd.DataFrame(scaled_data, columns=data.columns, index=data.index)\n",
        "    else:\n",
        "        processed_data = data\n",
        "    \n",
        "    return processed_data, None\n",
        "\n",
        "preprocessing_output = widgets.Output(layout=Layout(height='200px', overflow='auto'))\n",
        "\n",
        "# Feature selection widgets\n",
        "feature_selection = widgets.SelectMultiple(\n",
        "    options=[],\n",
        "    description='Select Features:',\n",
        "    disabled=True,\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='500px', height='150px')\n",
        ")\n",
        "\n",
        "def update_feature_selection():\n",
        "    \"\"\"Update feature selection options\"\"\"\n",
        "    if uploaded_data is not None:\n",
        "        feature_selection.options = uploaded_data.columns.tolist()\n",
        "        feature_selection.disabled = False\n",
        "        # Auto-select numeric columns\n",
        "        numeric_cols = get_numeric_columns(uploaded_data)\n",
        "        if numeric_cols:\n",
        "            feature_selection.value = tuple(numeric_cols[:min(10, len(numeric_cols))])\n",
        "    else:\n",
        "        feature_selection.options = []\n",
        "        feature_selection.disabled = True\n",
        "\n",
        "# Preprocessing options\n",
        "handle_missing_dropdown = widgets.Dropdown(\n",
        "    options=['drop', 'mean', 'median'],\n",
        "    value='drop',\n",
        "    description='Handle Missing:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='300px')\n",
        ")\n",
        "\n",
        "scale_features_checkbox = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Scale Features',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "scaler_type_dropdown = widgets.Dropdown(\n",
        "    options=['StandardScaler', 'MinMaxScaler'],\n",
        "    value='StandardScaler',\n",
        "    description='Scaler Type:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='300px')\n",
        ")\n",
        "\n",
        "encode_categorical_checkbox = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Encode Categorical',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "def apply_preprocessing(b):\n",
        "    \"\"\"Apply preprocessing\"\"\"\n",
        "    global processed_data, selected_features\n",
        "    \n",
        "    if uploaded_data is None:\n",
        "        print(\"Error: Please upload data first\")\n",
        "        return\n",
        "    \n",
        "    selected_features = list(feature_selection.value)\n",
        "    if len(selected_features) == 0:\n",
        "        print(\"Error: Please select at least one feature\")\n",
        "        return\n",
        "    \n",
        "    with preprocessing_output:\n",
        "        clear_output()\n",
        "        processed_data, error = preprocess_data(\n",
        "            uploaded_data,\n",
        "            selected_features,\n",
        "            handle_missing=handle_missing_dropdown.value,\n",
        "            scale_features=scale_features_checkbox.value,\n",
        "            scaler_type=scaler_type_dropdown.value,\n",
        "            encode_categorical=encode_categorical_checkbox.value\n",
        "        )\n",
        "        \n",
        "        if error:\n",
        "            print(f\"Error: {error}\")\n",
        "            return\n",
        "        \n",
        "        print(f\"✓ Preprocessing completed!\")\n",
        "        print(f\"Processed data shape: {processed_data.shape}\")\n",
        "        print(f\"Selected features: {len(selected_features)}\")\n",
        "        print(f\"\\nProcessed data preview:\")\n",
        "        display(processed_data.head())\n",
        "\n",
        "apply_preprocessing_btn = widgets.Button(\n",
        "    description='Apply Preprocessing',\n",
        "    button_style='primary',\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "apply_preprocessing_btn.on_click(apply_preprocessing)\n",
        "\n",
        "# Update feature selection when data is uploaded\n",
        "# This will be called when file_upload changes\n",
        "def on_data_uploaded_for_preprocessing():\n",
        "    update_feature_selection()\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>Data Preprocessing</h3>\"),\n",
        "    feature_selection,\n",
        "    widgets.HBox([\n",
        "        handle_missing_dropdown,\n",
        "        scaler_type_dropdown\n",
        "    ]),\n",
        "    widgets.HBox([\n",
        "        scale_features_checkbox,\n",
        "        encode_categorical_checkbox\n",
        "    ]),\n",
        "    apply_preprocessing_btn,\n",
        "    preprocessing_output\n",
        "]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Algorithm Selection & Configuration\n",
        "\n",
        "Select a clustering algorithm and configure its parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Algorithm Selection & Parameter Configuration\n",
        "\n",
        "ALGORITHMS = [\"K-Means\", \"DBSCAN\", \"HDBSCAN\", \"Hierarchical\", \"GMM\"]\n",
        "\n",
        "algorithm_dropdown = widgets.Dropdown(\n",
        "    options=ALGORITHMS,\n",
        "    description='Algorithm:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='300px')\n",
        ")\n",
        "\n",
        "# Parameter widgets (will be updated based on algorithm)\n",
        "parameter_container = widgets.VBox([])\n",
        "\n",
        "# K-Means parameters\n",
        "kmeans_n_clusters = widgets.IntText(\n",
        "    value=3,\n",
        "    description='n_clusters:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "\n",
        "# DBSCAN parameters\n",
        "dbscan_eps = widgets.FloatText(\n",
        "    value=0.5,\n",
        "    description='eps:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "\n",
        "dbscan_min_samples = widgets.IntText(\n",
        "    value=5,\n",
        "    description='min_samples:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "\n",
        "# HDBSCAN parameters\n",
        "hdbscan_min_cluster_size = widgets.IntText(\n",
        "    value=5,\n",
        "    description='min_cluster_size:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "\n",
        "hdbscan_min_samples = widgets.IntText(\n",
        "    value=3,\n",
        "    description='min_samples:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "\n",
        "# Hierarchical parameters\n",
        "hierarchical_n_clusters = widgets.IntText(\n",
        "    value=3,\n",
        "    description='n_clusters:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "\n",
        "hierarchical_linkage = widgets.Dropdown(\n",
        "    options=['ward', 'complete', 'average', 'single'],\n",
        "    value='ward',\n",
        "    description='linkage:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "\n",
        "# GMM parameters\n",
        "gmm_n_components = widgets.IntText(\n",
        "    value=3,\n",
        "    description='n_components:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "\n",
        "gmm_covariance_type = widgets.Dropdown(\n",
        "    options=['full', 'tied', 'diag', 'spherical'],\n",
        "    value='full',\n",
        "    description='covariance_type:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width='200px')\n",
        ")\n",
        "\n",
        "def update_parameter_widgets(change):\n",
        "    \"\"\"Update parameter widgets based on selected algorithm\"\"\"\n",
        "    algorithm = change['new']\n",
        "    \n",
        "    if algorithm == \"K-Means\":\n",
        "        parameter_container.children = [\n",
        "            widgets.HTML(\"<h4>K-Means Parameters</h4>\"),\n",
        "            kmeans_n_clusters\n",
        "        ]\n",
        "    elif algorithm == \"DBSCAN\":\n",
        "        parameter_container.children = [\n",
        "            widgets.HTML(\"<h4>DBSCAN Parameters</h4>\"),\n",
        "            dbscan_eps,\n",
        "            dbscan_min_samples\n",
        "        ]\n",
        "    elif algorithm == \"HDBSCAN\":\n",
        "        parameter_container.children = [\n",
        "            widgets.HTML(\"<h4>HDBSCAN Parameters</h4>\"),\n",
        "            hdbscan_min_cluster_size,\n",
        "            hdbscan_min_samples\n",
        "        ]\n",
        "    elif algorithm == \"Hierarchical\":\n",
        "        parameter_container.children = [\n",
        "            widgets.HTML(\"<h4>Hierarchical Clustering Parameters</h4>\"),\n",
        "            hierarchical_n_clusters,\n",
        "            hierarchical_linkage\n",
        "        ]\n",
        "    elif algorithm == \"GMM\":\n",
        "        parameter_container.children = [\n",
        "            widgets.HTML(\"<h4>Gaussian Mixture Model Parameters</h4>\"),\n",
        "            gmm_n_components,\n",
        "            gmm_covariance_type\n",
        "        ]\n",
        "\n",
        "algorithm_dropdown.observe(update_parameter_widgets, names='value')\n",
        "update_parameter_widgets({'new': algorithm_dropdown.value})\n",
        "\n",
        "# Apply use case defaults\n",
        "def apply_use_case_defaults():\n",
        "    \"\"\"Apply default parameters from use case\"\"\"\n",
        "    if current_use_case and current_use_case in use_cases_registry:\n",
        "        defaults = use_cases_registry[current_use_case].get(\"default_parameters\", {})\n",
        "        \n",
        "        if \"HDBSCAN\" in defaults:\n",
        "            hdbscan_min_cluster_size.value = defaults[\"HDBSCAN\"].get(\"min_cluster_size\", 5)\n",
        "            hdbscan_min_samples.value = defaults[\"HDBSCAN\"].get(\"min_samples\", 3)\n",
        "        \n",
        "        if \"DBSCAN\" in defaults:\n",
        "            dbscan_eps.value = defaults[\"DBSCAN\"].get(\"eps\", 0.5)\n",
        "            dbscan_min_samples.value = defaults[\"DBSCAN\"].get(\"min_samples\", 5)\n",
        "        \n",
        "        if \"K-Means\" in defaults:\n",
        "            kmeans_n_clusters.value = defaults[\"K-Means\"].get(\"n_clusters\", 3)\n",
        "\n",
        "# Show recommendations if available\n",
        "recommendation_info = widgets.HTML(\n",
        "    value=\"\",\n",
        "    layout=Layout(display='none', border='1px solid #2196F3', padding='10px', margin='10px 0px')\n",
        ")\n",
        "\n",
        "def show_recommendations():\n",
        "    \"\"\"Show algorithm recommendations from questionnaire\"\"\"\n",
        "    if questionnaire_answers:\n",
        "        recs, reasoning = recommend_algorithms(questionnaire_answers)\n",
        "        if recs:\n",
        "            rec_text = \"<h4>Recommended Algorithms (from questionnaire):</h4><ul>\"\n",
        "            for rec in recs:\n",
        "                rec_text += f\"<li><strong>{rec}</strong></li>\"\n",
        "            rec_text += \"</ul>\"\n",
        "            recommendation_info.value = rec_text\n",
        "            recommendation_info.layout.display = 'flex'\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>Algorithm Selection</h3>\"),\n",
        "    recommendation_info,\n",
        "    algorithm_dropdown,\n",
        "    parameter_container\n",
        "]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clustering Execution Functions\n",
        "\n",
        "clustering_output = widgets.Output(layout=Layout(height='300px', overflow='auto'))\n",
        "current_model = None\n",
        "cluster_labels = None\n",
        "\n",
        "def execute_kmeans(data, n_clusters):\n",
        "    \"\"\"Execute K-Means clustering\"\"\"\n",
        "    model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    labels = model.fit_predict(data)\n",
        "    return model, labels\n",
        "\n",
        "def execute_dbscan(data, eps, min_samples):\n",
        "    \"\"\"Execute DBSCAN clustering\"\"\"\n",
        "    model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = model.fit_predict(data)\n",
        "    return model, labels\n",
        "\n",
        "def execute_hdbscan(data, min_cluster_size, min_samples):\n",
        "    \"\"\"Execute HDBSCAN clustering\"\"\"\n",
        "    model = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
        "    labels = model.fit_predict(data)\n",
        "    return model, labels\n",
        "\n",
        "def execute_hierarchical(data, n_clusters, linkage):\n",
        "    \"\"\"Execute Hierarchical clustering\"\"\"\n",
        "    model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
        "    labels = model.fit_predict(data)\n",
        "    return model, labels\n",
        "\n",
        "def execute_gmm(data, n_components, covariance_type):\n",
        "    \"\"\"Execute Gaussian Mixture Model clustering\"\"\"\n",
        "    model = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=42)\n",
        "    labels = model.fit_predict(data)\n",
        "    return model, labels\n",
        "\n",
        "def run_clustering(b):\n",
        "    \"\"\"Execute clustering with selected algorithm and parameters\"\"\"\n",
        "    global current_model, cluster_labels, clustering_results\n",
        "    \n",
        "    if processed_data is None:\n",
        "        print(\"Error: Please preprocess data first\")\n",
        "        return\n",
        "    \n",
        "    algorithm = algorithm_dropdown.value\n",
        "    \n",
        "    with clustering_output:\n",
        "        clear_output()\n",
        "        \n",
        "        try:\n",
        "            data_array = processed_data.values\n",
        "            \n",
        "            if algorithm == \"K-Means\":\n",
        "                model, labels = execute_kmeans(data_array, kmeans_n_clusters.value)\n",
        "            elif algorithm == \"DBSCAN\":\n",
        "                model, labels = execute_dbscan(data_array, dbscan_eps.value, dbscan_min_samples.value)\n",
        "            elif algorithm == \"HDBSCAN\":\n",
        "                model, labels = execute_hdbscan(data_array, hdbscan_min_cluster_size.value, hdbscan_min_samples.value)\n",
        "            elif algorithm == \"Hierarchical\":\n",
        "                model, labels = execute_hierarchical(data_array, hierarchical_n_clusters.value, hierarchical_linkage.value)\n",
        "            elif algorithm == \"GMM\":\n",
        "                model, labels = execute_gmm(data_array, gmm_n_components.value, gmm_covariance_type.value)\n",
        "            else:\n",
        "                print(f\"Error: Unknown algorithm {algorithm}\")\n",
        "                return\n",
        "            \n",
        "            current_model = model\n",
        "            cluster_labels = labels\n",
        "            \n",
        "            # Store results\n",
        "            clustering_results = {\n",
        "                'algorithm': algorithm,\n",
        "                'model': model,\n",
        "                'labels': labels,\n",
        "                'n_clusters': len(set(labels)) - (1 if -1 in labels else 0),\n",
        "                'n_noise': int(np.sum(labels == -1)) if -1 in labels else 0\n",
        "            }\n",
        "            \n",
        "            print(f\"✓ Clustering completed successfully!\")\n",
        "            print(f\"Algorithm: {algorithm}\")\n",
        "            print(f\"Number of clusters: {clustering_results['n_clusters']}\")\n",
        "            if clustering_results['n_noise'] > 0:\n",
        "                print(f\"Number of noise points: {clustering_results['n_noise']}\")\n",
        "            \n",
        "            # Show cluster distribution\n",
        "            unique, counts = np.unique(labels, return_counts=True)\n",
        "            print(f\"\\nCluster distribution:\")\n",
        "            for cluster_id, count in zip(unique, counts):\n",
        "                if cluster_id == -1:\n",
        "                    print(f\"  Noise: {count} points\")\n",
        "                else:\n",
        "                    print(f\"  Cluster {cluster_id}: {count} points\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error during clustering: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "run_clustering_btn = widgets.Button(\n",
        "    description='Run Clustering',\n",
        "    button_style='success',\n",
        "    layout=Layout(width='200px', margin='10px 0px')\n",
        ")\n",
        "run_clustering_btn.on_click(run_clustering)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>Clustering Execution</h3>\"),\n",
        "    run_clustering_btn,\n",
        "    clustering_output\n",
        "]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization\n",
        "\n",
        "Visualize clustering results with interactive plots.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization Functions\n",
        "\n",
        "visualization_output = widgets.Output(layout=Layout(height='600px', overflow='auto'))\n",
        "\n",
        "def create_visualization():\n",
        "    \"\"\"Create visualization of clustering results\"\"\"\n",
        "    global visualization_output\n",
        "    \n",
        "    if processed_data is None or cluster_labels is None:\n",
        "        print(\"Error: Please run clustering first\")\n",
        "        return\n",
        "    \n",
        "    with visualization_output:\n",
        "        clear_output()\n",
        "        \n",
        "        # Reduce dimensions for visualization\n",
        "        if processed_data.shape[1] > 2:\n",
        "            pca = PCA(n_components=2, random_state=42)\n",
        "            data_2d = pca.fit_transform(processed_data.values)\n",
        "            explained_var = pca.explained_variance_ratio_.sum()\n",
        "            print(f\"Using PCA for visualization (explained variance: {explained_var:.2%})\")\n",
        "        else:\n",
        "            data_2d = processed_data.values\n",
        "            explained_var = 1.0\n",
        "        \n",
        "        # Create DataFrame for plotting\n",
        "        plot_df = pd.DataFrame({\n",
        "            'x': data_2d[:, 0],\n",
        "            'y': data_2d[:, 1],\n",
        "            'cluster': cluster_labels\n",
        "        })\n",
        "        \n",
        "        # Separate noise points if any\n",
        "        noise_mask = plot_df['cluster'] == -1\n",
        "        clustered_mask = ~noise_mask\n",
        "        \n",
        "        # Create plotly figure\n",
        "        fig = go.Figure()\n",
        "        \n",
        "        # Plot clusters\n",
        "        unique_clusters = sorted([c for c in plot_df['cluster'].unique() if c != -1])\n",
        "        colors = px.colors.qualitative.Set3\n",
        "        \n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            cluster_data = plot_df[plot_df['cluster'] == cluster_id]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=cluster_data['x'],\n",
        "                y=cluster_data['y'],\n",
        "                mode='markers',\n",
        "                name=f'Cluster {cluster_id}',\n",
        "                marker=dict(\n",
        "                    size=8,\n",
        "                    color=colors[i % len(colors)],\n",
        "                    opacity=0.7\n",
        "                )\n",
        "            ))\n",
        "        \n",
        "        # Plot noise points if any\n",
        "        if noise_mask.any():\n",
        "            noise_data = plot_df[noise_mask]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=noise_data['x'],\n",
        "                y=noise_data['y'],\n",
        "                mode='markers',\n",
        "                name='Noise',\n",
        "                marker=dict(\n",
        "                    size=6,\n",
        "                    color='gray',\n",
        "                    opacity=0.5,\n",
        "                    symbol='x'\n",
        "                )\n",
        "            ))\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title=f'Clustering Results - {clustering_results[\"algorithm\"]}',\n",
        "            xaxis_title='First Principal Component' if processed_data.shape[1] > 2 else processed_data.columns[0],\n",
        "            yaxis_title='Second Principal Component' if processed_data.shape[1] > 2 else processed_data.columns[1] if processed_data.shape[1] > 1 else 'Value',\n",
        "            hovermode='closest',\n",
        "            width=800,\n",
        "            height=600\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "        \n",
        "        # Also create matplotlib version for static display\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        \n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            cluster_data = plot_df[plot_df['cluster'] == cluster_id]\n",
        "            plt.scatter(cluster_data['x'], cluster_data['y'], \n",
        "                       label=f'Cluster {cluster_id}', \n",
        "                       alpha=0.7, s=50)\n",
        "        \n",
        "        if noise_mask.any():\n",
        "            noise_data = plot_df[noise_mask]\n",
        "            plt.scatter(noise_data['x'], noise_data['y'], \n",
        "                       label='Noise', \n",
        "                       color='gray', marker='x', alpha=0.5, s=30)\n",
        "        \n",
        "        plt.xlabel('First Principal Component' if processed_data.shape[1] > 2 else processed_data.columns[0])\n",
        "        plt.ylabel('Second Principal Component' if processed_data.shape[1] > 2 else processed_data.columns[1] if processed_data.shape[1] > 1 else 'Value')\n",
        "        plt.title(f'Clustering Results - {clustering_results[\"algorithm\"]}')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "visualize_btn = widgets.Button(\n",
        "    description='Visualize Results',\n",
        "    button_style='info',\n",
        "    layout=Layout(width='200px', margin='10px 0px')\n",
        ")\n",
        "visualize_btn.on_click(lambda b: create_visualization())\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>Visualization</h3>\"),\n",
        "    visualize_btn,\n",
        "    visualization_output\n",
        "]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Metrics & Analysis\n",
        "\n",
        "View clustering performance metrics and analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics Calculation\n",
        "\n",
        "metrics_output = widgets.Output(layout=Layout(height='400px', overflow='auto'))\n",
        "\n",
        "def calculate_metrics():\n",
        "    \"\"\"Calculate and display clustering metrics\"\"\"\n",
        "    global metrics_output\n",
        "    \n",
        "    if processed_data is None or cluster_labels is None:\n",
        "        print(\"Error: Please run clustering first\")\n",
        "        return\n",
        "    \n",
        "    with metrics_output:\n",
        "        clear_output()\n",
        "        \n",
        "        # Filter out noise points for metrics calculation\n",
        "        valid_mask = cluster_labels != -1\n",
        "        if valid_mask.sum() < 2:\n",
        "            print(\"Error: Not enough valid clusters for metrics calculation\")\n",
        "            return\n",
        "        \n",
        "        data_for_metrics = processed_data.values[valid_mask]\n",
        "        labels_for_metrics = cluster_labels[valid_mask]\n",
        "        \n",
        "        if len(set(labels_for_metrics)) < 2:\n",
        "            print(\"Error: Need at least 2 clusters for metrics calculation\")\n",
        "            return\n",
        "        \n",
        "        try:\n",
        "            # Calculate metrics\n",
        "            silhouette = silhouette_score(data_for_metrics, labels_for_metrics)\n",
        "            davies_bouldin = davies_bouldin_score(data_for_metrics, labels_for_metrics)\n",
        "            calinski_harabasz = calinski_harabasz_score(data_for_metrics, labels_for_metrics)\n",
        "            \n",
        "            # Cluster statistics\n",
        "            unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
        "            n_clusters = len([l for l in unique_labels if l != -1])\n",
        "            n_noise = int(np.sum(cluster_labels == -1))\n",
        "            \n",
        "            # Display metrics\n",
        "            print(\"=\" * 60)\n",
        "            print(\"CLUSTERING METRICS\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"\\nAlgorithm: {clustering_results['algorithm']}\")\n",
        "            print(f\"Number of clusters: {n_clusters}\")\n",
        "            if n_noise > 0:\n",
        "                print(f\"Number of noise points: {n_noise}\")\n",
        "            \n",
        "            print(f\"\\n--- Performance Metrics ---\")\n",
        "            print(f\"Silhouette Score: {silhouette:.4f}\")\n",
        "            print(f\"  (Range: -1 to 1, higher is better)\")\n",
        "            print(f\"Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
        "            print(f\"  (Lower is better)\")\n",
        "            print(f\"Calinski-Harabasz Index: {calinski_harabasz:.4f}\")\n",
        "            print(f\"  (Higher is better)\")\n",
        "            \n",
        "            print(f\"\\n--- Cluster Statistics ---\")\n",
        "            for label, count in zip(unique_labels, counts):\n",
        "                if label == -1:\n",
        "                    print(f\"Noise: {count} points ({count/len(cluster_labels)*100:.2f}%)\")\n",
        "                else:\n",
        "                    print(f\"Cluster {label}: {count} points ({count/len(cluster_labels)*100:.2f}%)\")\n",
        "            \n",
        "            # Use case-specific metrics\n",
        "            if current_use_case == \"intersight_alarms\":\n",
        "                print(f\"\\n--- Use Case Specific Analysis ---\")\n",
        "                print(f\"Total alarms: {len(cluster_labels)}\")\n",
        "                print(f\"Alarms grouped into {n_clusters} clusters\")\n",
        "                if n_noise > 0:\n",
        "                    print(f\"Unclustered alarms (noise): {n_noise}\")\n",
        "                print(f\"Average alarms per cluster: {(len(cluster_labels) - n_noise) / n_clusters:.2f}\")\n",
        "            \n",
        "            # Store metrics\n",
        "            clustering_results['metrics'] = {\n",
        "                'silhouette_score': silhouette,\n",
        "                'davies_bouldin_index': davies_bouldin,\n",
        "                'calinski_harabasz_index': calinski_harabasz,\n",
        "                'n_clusters': n_clusters,\n",
        "                'n_noise': n_noise\n",
        "            }\n",
        "            \n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating metrics: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "calculate_metrics_btn = widgets.Button(\n",
        "    description='Calculate Metrics',\n",
        "    button_style='info',\n",
        "    layout=Layout(width='200px', margin='10px 0px')\n",
        ")\n",
        "calculate_metrics_btn.on_click(lambda b: calculate_metrics())\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>Metrics & Analysis</h3>\"),\n",
        "    calculate_metrics_btn,\n",
        "    metrics_output\n",
        "]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Results\n",
        "\n",
        "Save clustering results, configurations, and visualizations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Results Saving Functions\n",
        "\n",
        "RESULTS_DIR = Path(\"results\")\n",
        "results_output = widgets.Output(layout=Layout(height='200px', overflow='auto'))\n",
        "\n",
        "def save_results():\n",
        "    \"\"\"Save clustering results to file\"\"\"\n",
        "    global results_output\n",
        "    \n",
        "    if clustering_results is None or len(clustering_results) == 0:\n",
        "        print(\"Error: No clustering results to save\")\n",
        "        return\n",
        "    \n",
        "    RESULTS_DIR.mkdir(exist_ok=True)\n",
        "    \n",
        "    with results_output:\n",
        "        clear_output()\n",
        "        \n",
        "        try:\n",
        "            # Create results dictionary\n",
        "            save_data = {\n",
        "                'use_case': current_use_case,\n",
        "                'use_case_name': use_cases_registry.get(current_use_case, {}).get('name', 'Unknown') if current_use_case else 'Unknown',\n",
        "                'algorithm': clustering_results['algorithm'],\n",
        "                'parameters': {},\n",
        "                'metrics': clustering_results.get('metrics', {}),\n",
        "                'cluster_info': {\n",
        "                    'n_clusters': clustering_results['n_clusters'],\n",
        "                    'n_noise': clustering_results['n_noise']\n",
        "                },\n",
        "                'questionnaire_answers': questionnaire_answers,\n",
        "                'selected_features': selected_features,\n",
        "                'preprocessing': {\n",
        "                    'handle_missing': handle_missing_dropdown.value,\n",
        "                    'scale_features': scale_features_checkbox.value,\n",
        "                    'scaler_type': scaler_type_dropdown.value,\n",
        "                    'encode_categorical': encode_categorical_checkbox.value\n",
        "                },\n",
        "                'timestamp': pd.Timestamp.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            # Add algorithm-specific parameters\n",
        "            algorithm = clustering_results['algorithm']\n",
        "            if algorithm == \"K-Means\":\n",
        "                save_data['parameters'] = {'n_clusters': kmeans_n_clusters.value}\n",
        "            elif algorithm == \"DBSCAN\":\n",
        "                save_data['parameters'] = {'eps': dbscan_eps.value, 'min_samples': dbscan_min_samples.value}\n",
        "            elif algorithm == \"HDBSCAN\":\n",
        "                save_data['parameters'] = {'min_cluster_size': hdbscan_min_cluster_size.value, 'min_samples': hdbscan_min_samples.value}\n",
        "            elif algorithm == \"Hierarchical\":\n",
        "                save_data['parameters'] = {'n_clusters': hierarchical_n_clusters.value, 'linkage': hierarchical_linkage.value}\n",
        "            elif algorithm == \"GMM\":\n",
        "                save_data['parameters'] = {'n_components': gmm_n_components.value, 'covariance_type': gmm_covariance_type.value}\n",
        "            \n",
        "            # Generate filename\n",
        "            timestamp_str = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            use_case_str = current_use_case if current_use_case else \"generic\"\n",
        "            filename = f\"{use_case_str}_{algorithm}_{timestamp_str}.json\"\n",
        "            filepath = RESULTS_DIR / filename\n",
        "            \n",
        "            # Save JSON\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(save_data, f, indent=2, default=str)\n",
        "            \n",
        "            # Save cluster labels to CSV\n",
        "            if uploaded_data is not None and cluster_labels is not None:\n",
        "                results_df = uploaded_data.copy()\n",
        "                results_df['cluster_label'] = cluster_labels\n",
        "                csv_filename = f\"{use_case_str}_{algorithm}_{timestamp_str}_labels.csv\"\n",
        "                csv_filepath = RESULTS_DIR / csv_filename\n",
        "                results_df.to_csv(csv_filepath, index=False)\n",
        "                print(f\"✓ Saved cluster labels to: {csv_filename}\")\n",
        "            \n",
        "            print(f\"✓ Results saved successfully!\")\n",
        "            print(f\"File: {filename}\")\n",
        "            print(f\"Location: {filepath}\")\n",
        "            print(f\"\\nSaved information:\")\n",
        "            print(f\"  - Use case: {save_data['use_case_name']}\")\n",
        "            print(f\"  - Algorithm: {save_data['algorithm']}\")\n",
        "            print(f\"  - Parameters: {save_data['parameters']}\")\n",
        "            if save_data['metrics']:\n",
        "                print(f\"  - Metrics: Silhouette={save_data['metrics'].get('silhouette_score', 'N/A'):.4f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error saving results: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "save_results_btn = widgets.Button(\n",
        "    description='Save Results',\n",
        "    button_style='success',\n",
        "    layout=Layout(width='200px', margin='10px 0px')\n",
        ")\n",
        "save_results_btn.on_click(lambda b: save_results())\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>Save Results</h3>\"),\n",
        "    save_results_btn,\n",
        "    results_output\n",
        "]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Integration & Auto-updates\n",
        "\n",
        "This section ensures components are properly connected and update automatically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Integration: Connect file upload to feature selection update\n",
        "\n",
        "def on_file_upload_for_integration(change):\n",
        "    \"\"\"Update feature selection when file is uploaded\"\"\"\n",
        "    if uploaded_data is not None:\n",
        "        update_feature_selection()\n",
        "\n",
        "# Re-observe file upload to trigger feature selection update\n",
        "if 'file_upload' in globals():\n",
        "    file_upload.observe(on_file_upload_for_integration, names='value')\n",
        "\n",
        "# Auto-update questionnaire when use case changes\n",
        "if 'use_case_dropdown' in globals():\n",
        "    use_case_dropdown.observe(on_use_case_change_for_questionnaire, names='value')\n",
        "\n",
        "# Apply use case defaults when use case is selected\n",
        "def on_use_case_selected_for_defaults(change):\n",
        "    \"\"\"Apply use case defaults when use case is selected\"\"\"\n",
        "    apply_use_case_defaults()\n",
        "    show_recommendations()\n",
        "\n",
        "if 'use_case_dropdown' in globals():\n",
        "    use_case_dropdown.observe(on_use_case_selected_for_defaults, names='value')\n",
        "\n",
        "print(\"✓ Integration complete - components are connected\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
